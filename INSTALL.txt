================================================================================
AMAZON SCRAPER API - INSTALLATION & DEPLOYMENT GUIDE
================================================================================

TABLE OF CONTENTS
-----------------
1. Local Development Setup
2. VPS Production Deployment (Always Running)
3. Configuration
4. Usage & API Endpoints
5. Troubleshooting


================================================================================
1. LOCAL DEVELOPMENT SETUP
================================================================================

PREREQUISITES
-------------
- Python 3.7 or higher
- pip (Python package installer)
- Microsoft Edge browser
- Internet connection


STEP 1: Install Python Dependencies
------------------------------------
Open Command Prompt/PowerShell in AmazonScraper directory:

    pip install -r requirements.txt

Required packages:
- flask==3.0.0
- flask-cors==4.0.0
- selenium==4.15.2
- beautifulsoup4==4.12.2
- webdriver-manager==4.0.1
- fake-useragent==1.4.0
- python-dotenv==1.0.0


STEP 2: Configure Environment
------------------------------
Copy .env.example to .env:

    copy .env.example .env

Edit .env and set your API key:

    API_HOST=127.0.0.1
    API_PORT=5000
    API_KEY=your_secret_api_key_here
    DEBUG_MODE=True
    ALLOWED_ORIGINS=http://localhost:8000
    HEADLESS_MODE=False


STEP 3: Run API Server
-----------------------
    python api_server.py

Server will start at: http://127.0.0.1:5000


STEP 4: Test API
----------------
Using curl:

    curl -X POST http://127.0.0.1:5000/api/scrape ^
      -H "X-API-Key: your_secret_api_key_here" ^
      -H "Content-Type: application/json" ^
      -d "{\"product_url\": \"https://www.amazon.in/dp/B0FMDNZ61S\"}"


================================================================================
2. VPS PRODUCTION DEPLOYMENT (ALWAYS RUNNING)
================================================================================

PREREQUISITES
-------------
- Ubuntu 20.04/22.04 or Debian 11/12
- Root or sudo access
- Domain name (optional but recommended)


STEP 1: Install System Dependencies
------------------------------------
    # Update system
    sudo apt update && sudo apt upgrade -y

    # Install Python and pip
    sudo apt install python3 python3-pip python3-venv -y

    # Install Microsoft Edge
    curl https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor > microsoft.gpg
    sudo install -o root -g root -m 644 microsoft.gpg /etc/apt/trusted.gpg.d/
    sudo sh -c 'echo "deb [arch=amd64] https://packages.microsoft.com/repos/edge stable main" > /etc/apt/sources.list.d/microsoft-edge.list'
    sudo apt update
    sudo apt install microsoft-edge-stable -y

    # Install nginx (reverse proxy)
    sudo apt install nginx -y


STEP 2: Create Application User
--------------------------------
    sudo useradd -m -s /bin/bash amazonscraper
    sudo su - amazonscraper


STEP 3: Deploy Application Files
---------------------------------
    # Create app directory
    mkdir -p /home/amazonscraper/app
    cd /home/amazonscraper/app

    # Upload these files (via FTP, SCP, or Git):
    # - api_server.py
    # - api_config.py
    # - requirements.txt
    # - scrapers/ (entire folder)
    # - .env.example

    # Create virtual environment
    python3 -m venv venv
    source venv/bin/activate

    # Install dependencies
    pip install -r requirements.txt


STEP 4: Configure Production Environment
-----------------------------------------
    nano .env

Set production settings:

    API_HOST=0.0.0.0
    API_PORT=5000
    API_KEY=1AqqRHyRhnlWzvljsvjD011dROrTeS3jqVxmqZHUFDqnbe1zLZ5bqxE5wVMVXgwF
    DEBUG_MODE=False
    ALLOWED_ORIGINS=https://yourdomain.com,https://www.yourdomain.com
    HEADLESS_MODE=True
    BROWSER_TIMEOUT=30

Exit user session:
    exit


STEP 5: Create Systemd Service (ALWAYS RUNNING)
------------------------------------------------
This makes the API run 24/7 and auto-restart on crashes/reboots.

Create service file:
    sudo nano /etc/systemd/system/amazon-scraper-api.service

Paste this configuration:

    [Unit]
    Description=Amazon Scraper API Service
    After=network.target

    [Service]
    Type=simple
    User=amazonscraper
    Group=amazonscraper
    WorkingDirectory=/home/amazonscraper/app
    Environment="PATH=/home/amazonscraper/app/venv/bin"
    ExecStart=/home/amazonscraper/app/venv/bin/python api_server.py
    Restart=always
    RestartSec=10
    StandardOutput=append:/home/amazonscraper/app/logs/output.log
    StandardError=append:/home/amazonscraper/app/logs/error.log

    [Install]
    WantedBy=multi-user.target

Create logs directory:
    sudo mkdir -p /home/amazonscraper/app/logs
    sudo chown -R amazonscraper:amazonscraper /home/amazonscraper/app

Enable and start service:
    sudo systemctl daemon-reload
    sudo systemctl enable amazon-scraper-api
    sudo systemctl start amazon-scraper-api
    sudo systemctl status amazon-scraper-api

The API is now running 24/7!


STEP 6: Configure Nginx Reverse Proxy
--------------------------------------
Create nginx config:
    sudo nano /etc/nginx/sites-available/amazon-scraper-api

Paste this:

    server {
        listen 80;
        server_name api.yourdomain.com;

        location / {
            proxy_pass http://127.0.0.1:5000;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            proxy_connect_timeout 300s;
            proxy_send_timeout 300s;
            proxy_read_timeout 300s;
        }

        # Security headers
        add_header X-Frame-Options "SAMEORIGIN" always;
        add_header X-Content-Type-Options "nosniff" always;
        add_header X-XSS-Protection "1; mode=block" always;
    }

Enable site:
    sudo ln -s /etc/nginx/sites-available/amazon-scraper-api /etc/nginx/sites-enabled/
    sudo nginx -t
    sudo systemctl restart nginx


STEP 7: Install SSL Certificate (Recommended)
----------------------------------------------
    # Install certbot
    sudo apt install certbot python3-certbot-nginx -y

    # Get free SSL certificate
    sudo certbot --nginx -d api.yourdomain.com

    # Test auto-renewal
    sudo certbot renew --dry-run

SSL will auto-renew every 90 days!


STEP 8: Configure Firewall
---------------------------
    sudo ufw allow 'Nginx Full'
    sudo ufw allow ssh
    sudo ufw enable


================================================================================
3. CONFIGURATION
================================================================================

SUPPORTED COUNTRIES (15 Amazon Marketplaces)
---------------------------------------------
ðŸ‡ºðŸ‡¸ United States    - amazon.com
ðŸ‡¨ðŸ‡¦ Canada           - amazon.ca
ðŸ‡²ðŸ‡½ Mexico           - amazon.com.mx
ðŸ‡§ðŸ‡· Brazil           - amazon.com.br
ðŸ‡¬ðŸ‡§ United Kingdom   - amazon.co.uk
ðŸ‡©ðŸ‡ª Germany          - amazon.de
ðŸ‡«ðŸ‡· France           - amazon.fr
ðŸ‡®ðŸ‡¹ Italy            - amazon.it
ðŸ‡ªðŸ‡¸ Spain            - amazon.es
ðŸ‡³ðŸ‡± Netherlands      - amazon.nl
ðŸ‡¦ðŸ‡ª UAE              - amazon.ae
ðŸ‡®ðŸ‡³ India            - amazon.in
ðŸ‡¯ðŸ‡µ Japan            - amazon.co.jp
ðŸ‡¦ðŸ‡º Australia        - amazon.com.au
ðŸ‡¸ðŸ‡¬ Singapore        - amazon.sg


ENVIRONMENT VARIABLES
---------------------
API_HOST        - Server bind address (0.0.0.0=public, 127.0.0.1=local only)
API_PORT        - Server port (default: 5000)
API_KEY         - Secret key for authentication
DEBUG_MODE      - Enable debug logging (True/False)
ALLOWED_ORIGINS - CORS allowed domains (comma-separated)
HEADLESS_MODE   - Run browser without GUI (True for production)
BROWSER_TIMEOUT - Browser operation timeout in seconds


================================================================================
4. USAGE & API ENDPOINTS
================================================================================

ENDPOINT 1: Health Check
-------------------------
GET /health

Response:
{
  "status": "healthy",
  "supported_countries": 15
}


ENDPOINT 2: Scrape Product
---------------------------
POST /api/scrape
Headers: X-API-Key: your_api_key_here
         Content-Type: application/json

Body:
{
  "product_url": "https://www.amazon.in/dp/ASIN"
}

Response (12 fields):
{
  "success": true,
  "data": {
    "asin": "B0FMDNZ61S",
    "merchant": "Amazon",
    "name": "Product Name",
    "category": "Category",
    "subcategory": "Subcategory",
    "brand": "Brand Name",
    "current_price": 1299.00,
    "original_price": 1999.00,
    "stock_status": "In Stock",
    "image_path": "https://...",
    "rating": 4.2,
    "review_count": 1850
  }
}


LARAVEL INTEGRATION
-------------------
use App\Services\AmazonScraperService;

$scraper = new AmazonScraperService();
$result = $scraper->scrapeProduct('https://www.amazon.in/dp/B0FMDNZ61S');

if ($result['success']) {
    $productData = $result['data'];
    // Process data...
}


================================================================================
5. TROUBLESHOOTING
================================================================================

SERVICE MANAGEMENT COMMANDS
---------------------------
# Check status
sudo systemctl status amazon-scraper-api

# View live logs
sudo journalctl -u amazon-scraper-api -f

# Restart service
sudo systemctl restart amazon-scraper-api

# Stop service
sudo systemctl stop amazon-scraper-api

# Start service
sudo systemctl start amazon-scraper-api


COMMON ISSUES
-------------

Issue: WebDriver not found
---------------------------
Solution: Webdriver-manager auto-downloads on first run. Ensure internet.


Issue: Browser crashes in headless mode
----------------------------------------
Solution: Add swap space (increase RAM):

    sudo fallocate -l 2G /swapfile
    sudo chmod 600 /swapfile
    sudo mkswap /swapfile
    sudo swapon /swapfile


Issue: Nginx 502 Bad Gateway
-----------------------------
Solution: Check if API is running:

    sudo systemctl status amazon-scraper-api
    netstat -tulpn | grep 5000


Issue: Permission denied errors
--------------------------------
Solution: Fix ownership:

    sudo chown -R amazonscraper:amazonscraper /home/amazonscraper/app


Issue: API timeout
------------------
Solution: Increase timeout in nginx and .env
- Nginx: proxy_read_timeout 300s;
- .env: BROWSER_TIMEOUT=60


PERFORMANCE OPTIMIZATION (HIGH TRAFFIC)
----------------------------------------

1. Use Gunicorn (production-grade server):

    pip install gunicorn
    gunicorn -w 4 -b 0.0.0.0:5000 api_server:app

Update systemd ExecStart line:
    ExecStart=/home/amazonscraper/app/venv/bin/gunicorn -w 4 -b 0.0.0.0:5000 api_server:app

2. Enable browser pooling (reuse browser instances)
3. Add Redis caching for frequently scraped products


LOG FILES
---------
- System logs: sudo journalctl -u amazon-scraper-api
- Output log: /home/amazonscraper/app/logs/output.log
- Error log: /home/amazonscraper/app/logs/error.log

View errors:
    tail -f /home/amazonscraper/app/logs/error.log


HEALTH CHECK
------------
    curl http://your-server-ip:5000/health

Should return: {"status": "healthy", "supported_countries": 15}


SERVICE INFO
------------
- Service name: amazon-scraper-api
- Default port: 5000
- Auto-restart: Enabled (10 second delay)
- Auto-start on boot: Enabled
- Runs 24/7: Yes

================================================================================
For support or issues, check logs first!
================================================================================
